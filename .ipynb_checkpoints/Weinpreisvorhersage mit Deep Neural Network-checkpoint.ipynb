{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import os \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf #to define our own metrics\n",
    "\n",
    "#Keras Word processing\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Embedding, Dense, Input, Flatten\n",
    "from keras.layers import Activation, Concatenate\n",
    "from keras.losses import mse, cosine_proximity #Loss functions\n",
    "from keras.optimizers import Adam \n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Sklearn to label words\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# Matplotlib to plot price and loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For text preprocessing \n",
    "import re\n",
    "import string\n",
    "from collections import Counter \n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#data visualization\n",
    "import matplotlib.cm as cm\n",
    "import missingno as msno\n",
    "from matplotlib import rcParams\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Ignore the system warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('original.csv Kopie 2.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five top records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns='Unnamed: 0')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five last records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coloumns/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lenght of data is', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types of all coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(data.sample(150930))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count of Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum(data.isnull().any(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count of all values in Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['price']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"price\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric features distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(20,20),bins = 20, color=\"#107009AA\")\n",
    "plt.title(\"Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description']=data['description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data on Descripcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations + english_punctuations\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove_repeating_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessPost for applying all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPost(text): \n",
    "\n",
    "    text = re.sub('@[^\\s]+', ' ', text)\n",
    "    \n",
    "\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying processPost function for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"] = data[\"description\"].apply(lambda x: processPost(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data[\"description\"] = data[\"description\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"]=data[\"description\"].apply(lambda x: [item for item in x if item not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for tokens in data[\"description\"] for word in tokens]\n",
    "\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 25 words in description text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_words = Counter(all_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(25):\n",
    "    words.append(letter)\n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.rainbow(np.linspace(0, 1, 10))\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "\n",
    "plt.title('Top words in description Text')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.barh(words, counts, color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static variables \n",
    "learning_rate= 1e-3\n",
    "batch_size= 128\n",
    "dataset_file_name = \"original.csv Kopie 2.xls\"\n",
    "vocab_size= 10000 #Specifying the words in the network\n",
    "max_seq_length = 170 #Limit the length of the evaluation text/number of characters. Uniform input sequence \n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the data for training with description and price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data preparation\n",
    "def prepare_dataset():\n",
    "    # Convert CSV content to Panda Data Frame\n",
    "    data = pd.read_csv(dataset_file_name)\n",
    "    data=data[['description', 'price']]\n",
    "    # data shuffeln/mischen\n",
    "    #data = data.sample(frac=1)\n",
    "    \n",
    "    #filter unknown values\n",
    "    data = data[pd.notnull(data[\"price\"])] #drop unknown values\n",
    "\n",
    "    # split\n",
    "    train_size=int(len(data)*0.7) #70% of the data are training data\n",
    "    validation_size=int(len(data)*0.15) \n",
    "    test_size=int(len(data)*0.15) \n",
    "    \n",
    "    # prepare features and labels\n",
    "    # train\n",
    "    x_train_description = data['description'][:train_size]\n",
    "    y_train_price = data['price'][:train_size]\n",
    "\n",
    "    # validation\n",
    "    x_val_description = data['description'][train_size:train_size+validation_size] #[0,7-0,85]\n",
    "    y_val_price = data['price'][train_size:train_size+validation_size]\n",
    "\n",
    "    # test\n",
    "    x_test_description = data['description'][train_size+validation_size:]\n",
    "    y_test_price = data['price'][train_size+validation_size:]\n",
    "\n",
    "    # Create a tokenizer so that the words in the description are trained\n",
    "    tokenize = Tokenizer(num_words=vocab_size, char_level=False) #Depth \n",
    "    tokenize.fit_on_texts(x_train_description) # we only train using the TrainSet, because it is the biggest\n",
    "\n",
    "\n",
    "    # create feature from embeddings (generation of vectors)\n",
    "    x_train_embed = tokenize.texts_to_sequences(x_train_description)  \n",
    "    x_val_embed = tokenize.texts_to_sequences(x_val_description)  \n",
    "    x_test_embed = tokenize.texts_to_sequences(x_test_description)\n",
    "\n",
    "    # Normalize the sequences (Because of different length of the vectors)\n",
    "    x_train_embed = pad_sequences(x_train_embed, maxlen=max_seq_length, padding=\"post\")  \n",
    "    x_val_embed = pad_sequences(x_val_embed, maxlen=max_seq_length, padding=\"post\")  \n",
    "    x_test_embed = pad_sequences(x_test_embed, maxlen=max_seq_length, padding=\"post\")  \n",
    "\n",
    "    # shapes of the sets\n",
    "    print(\"Train size: \", x_train_description.shape, y_train_price.shape)\n",
    "    print(\"Validation size: \", x_val_description.shape, y_val_price.shape)\n",
    "    print(\"Test size: \", x_test_description.shape, y_test_price.shape)\n",
    "\n",
    "\n",
    "    return (\n",
    "        (x_train_description, x_train_embed, y_train_price), \n",
    "        (x_val_description, x_val_embed, y_val_price), \n",
    "        (x_test_description, x_test_embed, y_test_price)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Training & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get_predictions for getting the predictions of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "def get_predictions():\n",
    "    model = load_model('wine_ann_model.npz', custom_objects={'price_difference': price_difference})\n",
    "    \n",
    "    # Dataset loading\n",
    "    (\n",
    "        (x_train_description, x_train_embed, y_train_price), \n",
    "        (x_val_description, x_val_embed, y_val_price), \n",
    "        (x_test_description,  x_test_embed, y_test_price)\n",
    "    ) = prepare_dataset()\n",
    "\n",
    "    \n",
    "    max_number_prediction = len(y_test_price)\n",
    "    diff = 0\n",
    "\n",
    "    predictions = model.predict({\n",
    "        'input_description': x_train_description,\n",
    "        'input_description_pad': x_test_embed\n",
    "    })\n",
    "\n",
    "    print(predictions)\n",
    "\n",
    "    for i in range(max_number_prediction):\n",
    "        actual_price = y_test_price.iloc[i]\n",
    "        price_predicted = predictions[i][0]\n",
    "        description = x_test_description.iloc[i]\n",
    "        \n",
    "        diff += np.abs(price_predicted - actual_price)\n",
    "\n",
    "        print(description)\n",
    "        print(\"Predicted Price: \", price_predicted, \"; Actual Price: \", actual_price)\n",
    "\n",
    "    print(\"Allgemeine differenz zwischen prediction und laben des Testset ist: \", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price_difference function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_difference(y_true, y_pred):\n",
    "    x = tf.expand_dims(y_true, -1)\n",
    "    y = tf.expand_dims(y_pred, -1)\n",
    "    diff = tf.subtract(x, y)\n",
    "    diff = tf.abs(diff)\n",
    "    diff = tf.reduce_sum(diff)\n",
    "    diff = tf.reduce_mean(diff)\n",
    "    return diff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_model():\n",
    "    #Input\n",
    "    \n",
    "    input_layer = Input(shape=(max_seq_length,), name=\"input_description_pad\")\n",
    "    \n",
    "    #Layer\n",
    "    x = Embedding(vocab_size, 8, input_length=max_seq_length)(input_layer)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=out)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train function for the training of deep model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer\n",
    "def train():\n",
    "    (\n",
    "        (x_train_description, x_train_embed, y_train_price), \n",
    "        (x_val_description,  x_val_embed, y_val_price), \n",
    "        (x_test_description, x_test_embed, y_test_price)\n",
    "    ) = prepare_dataset()\n",
    "    \n",
    "\n",
    "    \n",
    "    #Model definition \n",
    "    deep_model = build_deep_model()\n",
    "    model_ = Dense(1, name=\"price\")(deep_model.output)\n",
    "\n",
    "    inputs = deep_model.input\n",
    "    combined_model = Model(inputs=inputs, outputs=model_)\n",
    "    \n",
    "    #Compiling\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    combined_model.compile(loss=mse, optimizer=opt, metrics=['accuracy', price_difference])\n",
    "    combined_model.summary()\n",
    "    \n",
    "\n",
    "    \n",
    "    #Plot\n",
    "    history = combined_model.fit(x={\n",
    "        'input_description': x_train_description,\n",
    "        'input_description_pad': x_train_embed\n",
    "    }, y={\n",
    "        'price': y_train_price\n",
    "    },  epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        validation_data=({\n",
    "            'input_description': x_train_description,\n",
    "            'input_description_pad': x_val_embed\n",
    "        }, y_val_price)\n",
    "    )\n",
    "\n",
    "    #training  \n",
    "    loss_training = history.history['loss']\n",
    "    loss_val = history.history['val_loss']\n",
    "    pricediff_training = history.history['price_difference']\n",
    "    pricediff_val = history.history['val_price_difference']\n",
    "\n",
    "    # plotte ergebnisse\n",
    "    epochs_ = range(epochs)\n",
    "    plt.plot(epochs_, loss_training, label=\"training loss\")\n",
    "    plt.plot(epochs_, loss_val, label=\"validation loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epochs_, pricediff_training, label=\"training price difference\")\n",
    "    plt.plot(epochs_, pricediff_val, label=\"validation price difference\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    combined_model.save('wine_ann_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the Train function and Training the deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#If Model is not saved, than traing \n",
    "def main():\n",
    "    train()\n",
    "          \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling Trained model, get_predictions function and getting Predictions with trained model of Deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If model is not saved, then train\n",
    "def main():\n",
    "    if os.path.exists('wine_ann_model.npz'):\n",
    "        get_predictions()\n",
    "    else:\n",
    "        print('There is no trained model')\n",
    "    \n",
    "\n",
    "        epo\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
